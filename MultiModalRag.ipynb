{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99ee6453ceb049ceb3bcb411fbb9a2d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c34c5efdafec45b7b5a38647abda9887",
              "IPY_MODEL_5e595a7129b04c2094fd5ff64ba19b86",
              "IPY_MODEL_3eba769ae51e4b2f960a85e391af4d45"
            ],
            "layout": "IPY_MODEL_7347fb816cb14261953543e7b6c142ea"
          }
        },
        "c34c5efdafec45b7b5a38647abda9887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f93c88b28c844e82be6215fdcbbeb9a1",
            "placeholder": "​",
            "style": "IPY_MODEL_44e3a19477fc47b3b9b9439a5359c943",
            "value": "config.json: "
          }
        },
        "5e595a7129b04c2094fd5ff64ba19b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e61ca7d59efa40a8a70ffd3e2e1b7d4e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9befe5cd3b0f4927b7f4c68c6618bcbe",
            "value": 1
          }
        },
        "3eba769ae51e4b2f960a85e391af4d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dccdf4486e75418596a3bb7995a3160d",
            "placeholder": "​",
            "style": "IPY_MODEL_d2a8cc4ca34843f4a53ffef9a0e393bb",
            "value": " 4.19k/? [00:00&lt;00:00, 270kB/s]"
          }
        },
        "7347fb816cb14261953543e7b6c142ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f93c88b28c844e82be6215fdcbbeb9a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44e3a19477fc47b3b9b9439a5359c943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e61ca7d59efa40a8a70ffd3e2e1b7d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9befe5cd3b0f4927b7f4c68c6618bcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dccdf4486e75418596a3bb7995a3160d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2a8cc4ca34843f4a53ffef9a0e393bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "784738f33edc4d50ae88e55eff15d247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b386b8734b8c41829eeef413347f87df",
              "IPY_MODEL_f4acd945cac4486a8183d5eece8a7de5",
              "IPY_MODEL_2b7ef5192779439b8c0562a0134d807a"
            ],
            "layout": "IPY_MODEL_b907d2ba147b46c4a83b9169308f0d54"
          }
        },
        "b386b8734b8c41829eeef413347f87df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcda65be0c6c4e9286accede80aa3667",
            "placeholder": "​",
            "style": "IPY_MODEL_7f6e2be573cd4a168fc300a0c21ad25b",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "f4acd945cac4486a8183d5eece8a7de5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6056328df58b4955971ad4c0c424a3e0",
            "max": 605247071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf7af5f14ce444fbb497a69cf6e530f6",
            "value": 605247071
          }
        },
        "2b7ef5192779439b8c0562a0134d807a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50dbcf0330104e22890967b1d07f4742",
            "placeholder": "​",
            "style": "IPY_MODEL_df2a3927685642259fe5ddad212c8588",
            "value": " 605M/605M [00:13&lt;00:00, 193MB/s]"
          }
        },
        "b907d2ba147b46c4a83b9169308f0d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcda65be0c6c4e9286accede80aa3667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f6e2be573cd4a168fc300a0c21ad25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6056328df58b4955971ad4c0c424a3e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf7af5f14ce444fbb497a69cf6e530f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50dbcf0330104e22890967b1d07f4742": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df2a3927685642259fe5ddad212c8588": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e09996547a04d4184342ff6845f0547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f287aa644074491a0998c7bef500b06",
              "IPY_MODEL_89cc4bf50bc6437f829c0201c283093b",
              "IPY_MODEL_be4a2ead589142768b7d3c8fd49487be"
            ],
            "layout": "IPY_MODEL_47aace188f194b2f9da0a19b2ccf1b18"
          }
        },
        "7f287aa644074491a0998c7bef500b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6694dda3730411b9feaf2138b8ef0c7",
            "placeholder": "​",
            "style": "IPY_MODEL_fd3e4ad4b379495da5752348423d6b03",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "89cc4bf50bc6437f829c0201c283093b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d7f7ad0b5b745d983b29b9cf76198e4",
            "max": 316,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b33ba74601c242bda6b78d1dae1476ed",
            "value": 316
          }
        },
        "be4a2ead589142768b7d3c8fd49487be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6f6b8c5890b4c15948b36b76d741717",
            "placeholder": "​",
            "style": "IPY_MODEL_a6cff0919f09488f99e4adbb466a0664",
            "value": " 316/316 [00:00&lt;00:00, 34.2kB/s]"
          }
        },
        "47aace188f194b2f9da0a19b2ccf1b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6694dda3730411b9feaf2138b8ef0c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd3e4ad4b379495da5752348423d6b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d7f7ad0b5b745d983b29b9cf76198e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b33ba74601c242bda6b78d1dae1476ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6f6b8c5890b4c15948b36b76d741717": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6cff0919f09488f99e4adbb466a0664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63a062dfbbbb4448ba120988807dbf65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2cf0c17828d4fcbaf1a088443db6344",
              "IPY_MODEL_fc4e7c91b97c4ae3a24d23c9bdf296ac",
              "IPY_MODEL_fefcf1ef6b9042a092d73a886ae8d0f2"
            ],
            "layout": "IPY_MODEL_c7876fb9a2d443878fedfe2313cc26b3"
          }
        },
        "e2cf0c17828d4fcbaf1a088443db6344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31e8a8a75a134a82a86165babb7e92ca",
            "placeholder": "​",
            "style": "IPY_MODEL_1549897127b943039a47f4225ae58501",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "fc4e7c91b97c4ae3a24d23c9bdf296ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc470bf72ad14cce88326d76e98ea75a",
            "max": 592,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f03c5b0b98fe4736bee1c408612937de",
            "value": 592
          }
        },
        "fefcf1ef6b9042a092d73a886ae8d0f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0270ff490144e978fbd2d9350bd3b8e",
            "placeholder": "​",
            "style": "IPY_MODEL_9b797e9e11dc43989cec5196ed59d1d8",
            "value": " 592/592 [00:00&lt;00:00, 56.4kB/s]"
          }
        },
        "c7876fb9a2d443878fedfe2313cc26b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e8a8a75a134a82a86165babb7e92ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1549897127b943039a47f4225ae58501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc470bf72ad14cce88326d76e98ea75a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f03c5b0b98fe4736bee1c408612937de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0270ff490144e978fbd2d9350bd3b8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b797e9e11dc43989cec5196ed59d1d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3640abb5a9d4c42872df00417f165a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f0785cbef724934a0e916fe2b72e576",
              "IPY_MODEL_4e8c5d755625465dab0fd89aecdbd6a0",
              "IPY_MODEL_94a64db9b3c841fe8a6f46d24750447d"
            ],
            "layout": "IPY_MODEL_eafd6ba074ff43fa8c28e4fd7dca05de"
          }
        },
        "4f0785cbef724934a0e916fe2b72e576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a5f6e9251514860962645da89459f2f",
            "placeholder": "​",
            "style": "IPY_MODEL_e629d9cab8614b6bae93e852e6703cdd",
            "value": "model.safetensors: 100%"
          }
        },
        "4e8c5d755625465dab0fd89aecdbd6a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7d223afb43d433e84774666f389a241",
            "max": 605157884,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ecb5cde58e145cab62f6a6c7a62cb54",
            "value": 605157884
          }
        },
        "94a64db9b3c841fe8a6f46d24750447d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa37c87a363f4ca5b8695012838587c2",
            "placeholder": "​",
            "style": "IPY_MODEL_5ff3fe62720b4803948c34d8f11e55f3",
            "value": " 605M/605M [00:09&lt;00:00, 48.2MB/s]"
          }
        },
        "eafd6ba074ff43fa8c28e4fd7dca05de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a5f6e9251514860962645da89459f2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e629d9cab8614b6bae93e852e6703cdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7d223afb43d433e84774666f389a241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ecb5cde58e145cab62f6a6c7a62cb54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa37c87a363f4ca5b8695012838587c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ff3fe62720b4803948c34d8f11e55f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9346409e972f4caba7e2925dd99c2594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ec55e08417642df89115e49dbad81ea",
              "IPY_MODEL_46c10822b5d0461597c8a6bb4ceee992",
              "IPY_MODEL_05f9d718945045d988230f077db4f518"
            ],
            "layout": "IPY_MODEL_d176b513fb1641f994f7a12c817209c1"
          }
        },
        "4ec55e08417642df89115e49dbad81ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d8b5c85597947d896866f8d1ca002b4",
            "placeholder": "​",
            "style": "IPY_MODEL_3a1e95245439414cba0658fb2aed9bc5",
            "value": "vocab.json: "
          }
        },
        "46c10822b5d0461597c8a6bb4ceee992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8463d4d702742cbbe67ec84fe0919d1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a167089a3b345f083bd1ec8411fefe6",
            "value": 1
          }
        },
        "05f9d718945045d988230f077db4f518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edddf2cc10a347279c1ac82f53229234",
            "placeholder": "​",
            "style": "IPY_MODEL_cd4e20b339a44517bac09fc2f84b784a",
            "value": " 862k/? [00:00&lt;00:00, 17.9MB/s]"
          }
        },
        "d176b513fb1641f994f7a12c817209c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d8b5c85597947d896866f8d1ca002b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a1e95245439414cba0658fb2aed9bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8463d4d702742cbbe67ec84fe0919d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4a167089a3b345f083bd1ec8411fefe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "edddf2cc10a347279c1ac82f53229234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd4e20b339a44517bac09fc2f84b784a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fd36c20b361453799813aee880a96eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f59f977e2d084064b81e88c30bf92f39",
              "IPY_MODEL_608aee79ae86416da84802ac53ee35c3",
              "IPY_MODEL_2334a76ab5584d15b732feedb030ec0c"
            ],
            "layout": "IPY_MODEL_a958d6768cb7409e93666235077a2836"
          }
        },
        "f59f977e2d084064b81e88c30bf92f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dc5213e071b475690ce99db3635a3db",
            "placeholder": "​",
            "style": "IPY_MODEL_d591f7a1ceb74e3c871ceba740e8c744",
            "value": "merges.txt: "
          }
        },
        "608aee79ae86416da84802ac53ee35c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68c51347fa9d440e8a82c3b17bbde93e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35837773a3764bdfa757c2016d323a45",
            "value": 1
          }
        },
        "2334a76ab5584d15b732feedb030ec0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb948b9a969a48b2bc8a102c8b300df3",
            "placeholder": "​",
            "style": "IPY_MODEL_35f29a3914e94a0c975c22e996a6c990",
            "value": " 525k/? [00:00&lt;00:00, 31.9MB/s]"
          }
        },
        "a958d6768cb7409e93666235077a2836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dc5213e071b475690ce99db3635a3db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d591f7a1ceb74e3c871ceba740e8c744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68c51347fa9d440e8a82c3b17bbde93e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "35837773a3764bdfa757c2016d323a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb948b9a969a48b2bc8a102c8b300df3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35f29a3914e94a0c975c22e996a6c990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9da6a978db734d5ca7da63b4be2b908f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a90b891465f4f4db7fe79ddf2d481ab",
              "IPY_MODEL_eb475c5dca874c77a3f986866e9997d5",
              "IPY_MODEL_44d83393e2b247d096d8401c5467d372"
            ],
            "layout": "IPY_MODEL_bd9dfe95fd114e9b9f87b30a76d329d8"
          }
        },
        "7a90b891465f4f4db7fe79ddf2d481ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_383e992b931844a99e4f17e5330ef722",
            "placeholder": "​",
            "style": "IPY_MODEL_5e1430b9fc4a45b7a5e1e15cc6079b62",
            "value": "tokenizer.json: "
          }
        },
        "eb475c5dca874c77a3f986866e9997d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88fd98b6dd414f4cac2f363ece87351d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d997e2cfdd84ad1a5257668643ecd7d",
            "value": 1
          }
        },
        "44d83393e2b247d096d8401c5467d372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53bc33c90f25422eb17fec61fd74a635",
            "placeholder": "​",
            "style": "IPY_MODEL_e7ec6e26da3d4d09ad49d86e02d95b2e",
            "value": " 2.22M/? [00:00&lt;00:00, 66.1MB/s]"
          }
        },
        "bd9dfe95fd114e9b9f87b30a76d329d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "383e992b931844a99e4f17e5330ef722": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e1430b9fc4a45b7a5e1e15cc6079b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88fd98b6dd414f4cac2f363ece87351d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0d997e2cfdd84ad1a5257668643ecd7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53bc33c90f25422eb17fec61fd74a635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7ec6e26da3d4d09ad49d86e02d95b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4970033cb2b84cfa804afc8a53c14d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c6080de00c54ec3ab49de7ded7bcb8c",
              "IPY_MODEL_55709969c4694e0fbbbfda8ea94e62cc",
              "IPY_MODEL_9840c9ad077c43e3853bf655915c4f2e"
            ],
            "layout": "IPY_MODEL_065b7d5d19e54ca388cd6fb843df73b0"
          }
        },
        "5c6080de00c54ec3ab49de7ded7bcb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f69be6b887e43c3a218641f243514cb",
            "placeholder": "​",
            "style": "IPY_MODEL_1fe0562eb9ab4f278dad7514e8f89afb",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "55709969c4694e0fbbbfda8ea94e62cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_740220f318bb4545816de8cdaffc64da",
            "max": 389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d00c45150ff48559313cee228f56be2",
            "value": 389
          }
        },
        "9840c9ad077c43e3853bf655915c4f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24b6734fb5a84d6a8c397a3ef83653b1",
            "placeholder": "​",
            "style": "IPY_MODEL_eac8fa42013c4edc8971603719162823",
            "value": " 389/389 [00:00&lt;00:00, 11.1kB/s]"
          }
        },
        "065b7d5d19e54ca388cd6fb843df73b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f69be6b887e43c3a218641f243514cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fe0562eb9ab4f278dad7514e8f89afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "740220f318bb4545816de8cdaffc64da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d00c45150ff48559313cee228f56be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24b6734fb5a84d6a8c397a3ef83653b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eac8fa42013c4edc8971603719162823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4c9ddf00",
        "outputId": "d63a7722-0e88-487c-a532-1bc73e66d334"
      },
      "source": [
        "!pip install PyMuPDF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4815031f",
        "outputId": "8cd35d02-29ce-4654-aa7d-e2f13b3af1ff"
      },
      "source": [
        "!pip install langchain-community"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.72)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-iM-9FYxTtx"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "from langchain_core.documents import Document\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.messages import HumanMessage\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import base64\n",
        "import io\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Clip Model\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "## set up the environment\n",
        "# Safely get the API key\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if openai_api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "else:\n",
        "    print(\"OPENAI_API_KEY not found. Please set it in Colab secrets.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"  # Placeholder\n",
        "\n",
        "\n",
        "### initialize the Clip Model for unified embeddings\n",
        "clip_model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "99ee6453ceb049ceb3bcb411fbb9a2d2",
            "c34c5efdafec45b7b5a38647abda9887",
            "5e595a7129b04c2094fd5ff64ba19b86",
            "3eba769ae51e4b2f960a85e391af4d45",
            "7347fb816cb14261953543e7b6c142ea",
            "f93c88b28c844e82be6215fdcbbeb9a1",
            "44e3a19477fc47b3b9b9439a5359c943",
            "e61ca7d59efa40a8a70ffd3e2e1b7d4e",
            "9befe5cd3b0f4927b7f4c68c6618bcbe",
            "dccdf4486e75418596a3bb7995a3160d",
            "d2a8cc4ca34843f4a53ffef9a0e393bb",
            "784738f33edc4d50ae88e55eff15d247",
            "b386b8734b8c41829eeef413347f87df",
            "f4acd945cac4486a8183d5eece8a7de5",
            "2b7ef5192779439b8c0562a0134d807a",
            "b907d2ba147b46c4a83b9169308f0d54",
            "dcda65be0c6c4e9286accede80aa3667",
            "7f6e2be573cd4a168fc300a0c21ad25b",
            "6056328df58b4955971ad4c0c424a3e0",
            "bf7af5f14ce444fbb497a69cf6e530f6",
            "50dbcf0330104e22890967b1d07f4742",
            "df2a3927685642259fe5ddad212c8588",
            "8e09996547a04d4184342ff6845f0547",
            "7f287aa644074491a0998c7bef500b06",
            "89cc4bf50bc6437f829c0201c283093b",
            "be4a2ead589142768b7d3c8fd49487be",
            "47aace188f194b2f9da0a19b2ccf1b18",
            "b6694dda3730411b9feaf2138b8ef0c7",
            "fd3e4ad4b379495da5752348423d6b03",
            "2d7f7ad0b5b745d983b29b9cf76198e4",
            "b33ba74601c242bda6b78d1dae1476ed",
            "f6f6b8c5890b4c15948b36b76d741717",
            "a6cff0919f09488f99e4adbb466a0664",
            "63a062dfbbbb4448ba120988807dbf65",
            "e2cf0c17828d4fcbaf1a088443db6344",
            "fc4e7c91b97c4ae3a24d23c9bdf296ac",
            "fefcf1ef6b9042a092d73a886ae8d0f2",
            "c7876fb9a2d443878fedfe2313cc26b3",
            "31e8a8a75a134a82a86165babb7e92ca",
            "1549897127b943039a47f4225ae58501",
            "bc470bf72ad14cce88326d76e98ea75a",
            "f03c5b0b98fe4736bee1c408612937de",
            "b0270ff490144e978fbd2d9350bd3b8e",
            "9b797e9e11dc43989cec5196ed59d1d8",
            "f3640abb5a9d4c42872df00417f165a9",
            "4f0785cbef724934a0e916fe2b72e576",
            "4e8c5d755625465dab0fd89aecdbd6a0",
            "94a64db9b3c841fe8a6f46d24750447d",
            "eafd6ba074ff43fa8c28e4fd7dca05de",
            "8a5f6e9251514860962645da89459f2f",
            "e629d9cab8614b6bae93e852e6703cdd",
            "b7d223afb43d433e84774666f389a241",
            "1ecb5cde58e145cab62f6a6c7a62cb54",
            "fa37c87a363f4ca5b8695012838587c2",
            "5ff3fe62720b4803948c34d8f11e55f3",
            "9346409e972f4caba7e2925dd99c2594",
            "4ec55e08417642df89115e49dbad81ea",
            "46c10822b5d0461597c8a6bb4ceee992",
            "05f9d718945045d988230f077db4f518",
            "d176b513fb1641f994f7a12c817209c1",
            "0d8b5c85597947d896866f8d1ca002b4",
            "3a1e95245439414cba0658fb2aed9bc5",
            "b8463d4d702742cbbe67ec84fe0919d1",
            "4a167089a3b345f083bd1ec8411fefe6",
            "edddf2cc10a347279c1ac82f53229234",
            "cd4e20b339a44517bac09fc2f84b784a",
            "8fd36c20b361453799813aee880a96eb",
            "f59f977e2d084064b81e88c30bf92f39",
            "608aee79ae86416da84802ac53ee35c3",
            "2334a76ab5584d15b732feedb030ec0c",
            "a958d6768cb7409e93666235077a2836",
            "0dc5213e071b475690ce99db3635a3db",
            "d591f7a1ceb74e3c871ceba740e8c744",
            "68c51347fa9d440e8a82c3b17bbde93e",
            "35837773a3764bdfa757c2016d323a45",
            "eb948b9a969a48b2bc8a102c8b300df3",
            "35f29a3914e94a0c975c22e996a6c990",
            "9da6a978db734d5ca7da63b4be2b908f",
            "7a90b891465f4f4db7fe79ddf2d481ab",
            "eb475c5dca874c77a3f986866e9997d5",
            "44d83393e2b247d096d8401c5467d372",
            "bd9dfe95fd114e9b9f87b30a76d329d8",
            "383e992b931844a99e4f17e5330ef722",
            "5e1430b9fc4a45b7a5e1e15cc6079b62",
            "88fd98b6dd414f4cac2f363ece87351d",
            "0d997e2cfdd84ad1a5257668643ecd7d",
            "53bc33c90f25422eb17fec61fd74a635",
            "e7ec6e26da3d4d09ad49d86e02d95b2e",
            "4970033cb2b84cfa804afc8a53c14d11",
            "5c6080de00c54ec3ab49de7ded7bcb8c",
            "55709969c4694e0fbbbfda8ea94e62cc",
            "9840c9ad077c43e3853bf655915c4f2e",
            "065b7d5d19e54ca388cd6fb843df73b0",
            "1f69be6b887e43c3a218641f243514cb",
            "1fe0562eb9ab4f278dad7514e8f89afb",
            "740220f318bb4545816de8cdaffc64da",
            "1d00c45150ff48559313cee228f56be2",
            "24b6734fb5a84d6a8c397a3ef83653b1",
            "eac8fa42013c4edc8971603719162823"
          ]
        },
        "id": "xXVqPtwXyCFy",
        "outputId": "f9b68804-1440-4f89-abbb-8cfed88d9fa4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99ee6453ceb049ceb3bcb411fbb9a2d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "784738f33edc4d50ae88e55eff15d247"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e09996547a04d4184342ff6845f0547"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63a062dfbbbb4448ba120988807dbf65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3640abb5a9d4c42872df00417f165a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9346409e972f4caba7e2925dd99c2594"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fd36c20b361453799813aee880a96eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9da6a978db734d5ca7da63b4be2b908f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4970033cb2b84cfa804afc8a53c14d11"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Embedding functions\n",
        "def embed_image(image_data):\n",
        "    \"\"\"Embed image using CLIP\"\"\"\n",
        "    if isinstance(image_data, str):  # If path\n",
        "        image = Image.open(image_data).convert(\"RGB\")\n",
        "    else:  # If PIL Image\n",
        "        image = image_data\n",
        "\n",
        "    inputs=clip_processor(images=image,return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        features = clip_model.get_image_features(**inputs)\n",
        "        # Normalize embeddings to unit vector\n",
        "        features = features / features.norm(dim=-1, keepdim=True)\n",
        "        return features.squeeze().numpy()\n",
        "\n",
        "def embed_text(text):\n",
        "    \"\"\"Embed text using CLIP.\"\"\"\n",
        "    inputs = clip_processor(\n",
        "        text=text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=77  # CLIP's max token length\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        features = clip_model.get_text_features(**inputs)\n",
        "        # Normalize embeddings\n",
        "        features = features / features.norm(dim=-1, keepdim=True)\n",
        "        return features.squeeze().numpy()"
      ],
      "metadata": {
        "id": "SkVDxiDQyd53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Process PDF\n",
        "pdf_path=\"/content/drive/MyDrive/Floorplan_Using_GNNs (2).pdf\"\n",
        "doc=fitz.open(pdf_path)\n",
        "# Storage for all documents and embeddings\n",
        "all_docs = []\n",
        "all_embeddings = []\n",
        "image_data_store = {}  # Store actual image data for LLM\n",
        "\n",
        "# Text splitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
      ],
      "metadata": {
        "id": "Y3zPhNB4_RxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q68wk_f7APpc",
        "outputId": "0c8f6893-2f55-436b-c0e0-889f75c570ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document('/content/drive/MyDrive/Floorplan_Using_GNNs (2).pdf')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,page in enumerate(doc):\n",
        "    ## process text\n",
        "    text=page.get_text()\n",
        "    if text.strip():\n",
        "        ##create temporary document for splitting\n",
        "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
        "        text_chunks = splitter.split_documents([temp_doc])\n",
        "\n",
        "        #Embed each chunk using CLIP\n",
        "        for chunk in text_chunks:\n",
        "            embedding = embed_text(chunk.page_content)\n",
        "            all_embeddings.append(embedding)\n",
        "            all_docs.append(chunk)\n",
        "\n",
        "\n",
        "\n",
        "    ## process images\n",
        "    ##Three Important Actions:\n",
        "\n",
        "    ##Convert PDF image to PIL format\n",
        "    ##Store as base64 for GPT-4V (which needs base64 images)\n",
        "    ##Create CLIP embedding for retrieval\n",
        "\n",
        "    for img_index, img in enumerate(page.get_images(full=True)):\n",
        "        try:\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            image_bytes = base_image[\"image\"]\n",
        "\n",
        "            # Convert to PIL Image\n",
        "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "            # Create unique identifier\n",
        "            image_id = f\"page_{i}_img_{img_index}\"\n",
        "\n",
        "            # Store image as base64 for later use with GPT-4V\n",
        "            buffered = io.BytesIO()\n",
        "            pil_image.save(buffered, format=\"PNG\")\n",
        "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
        "            image_data_store[image_id] = img_base64\n",
        "\n",
        "            # Embed image using CLIP\n",
        "            embedding = embed_image(pil_image)\n",
        "            all_embeddings.append(embedding)\n",
        "\n",
        "            # Create document for image\n",
        "            image_doc = Document(\n",
        "                page_content=f\"[Image: {image_id}]\",\n",
        "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
        "            )\n",
        "            all_docs.append(image_doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "doc.close()"
      ],
      "metadata": {
        "id": "gspNyD_cAbIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du41eMmJAlJc",
        "outputId": "5b89298a-57a7-4896-e144-1532b2db0471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 0, 'type': 'text'}, page_content='Residential Floor Plan Generation Using Graph\\nNeural Networks (GNNs)\\nAnkur Jain, Nikhil Kumar Singh, Kale Vishwajit Shankarrao, Dr Nilesh Bhosle\\nDepartment of AIML,NIMS Institute of Engineering Technology, Jaipur, India\\nEmail: jankur168@gmail.com\\nAbstract—This paper presents an automated residential floor\\nplan generation system using deep learning techniques, partic-\\nularly Graph Neural Networks and Graph Attention Networks.\\nTraditional floor plan creation is a manual, time-intensive process'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='Traditional floor plan creation is a manual, time-intensive process\\nrequiring architectural expertise, making it less accessible to\\nnon-experts. The proposed approach streamlines this process\\nby converting user-defined spatial requirements into structured\\ngraph representations where rooms are treated as nodes and\\nspatial connections as edges. Initially, a Convolutional Neural\\nNetwork predicts room centroids based on user inputs and\\nboundary maps. These centroid-level features are then processed'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='boundary maps. These centroid-level features are then processed\\nby a custom attention-based model, which refines room sizes\\nby capturing inter-room dependencies and enhancing layout\\ncoherence. The model is trained on the RPlan dataset, which\\nhas been converted into a geometric and graph-based format. A\\nkey strength of this work is its integration with a user-friendly\\nStreamlit-based interface, enabling real-time floor plan visualiza-'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='Streamlit-based interface, enabling real-time floor plan visualiza-\\ntion. Experimental results demonstrate substantial improvements\\nover baseline methods, validating the effectiveness of our deep\\nlearning pipeline for residential architectural design automation.\\nIndex Terms—Graph Neural Networks, Floor Plan Genera-\\ntion,Graph Attention Networks , RPlan Dataset, Convolutional\\nNeural Network , Residential architectural design automation.\\nI. INTRODUCTION'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='Neural Network , Residential architectural design automation.\\nI. INTRODUCTION\\nThe dream of designing personalized residential spaces is\\noften hindered by the technical expertise required in archi-\\ntectural drafting. For non-experts, visualizing and creating\\na functional home layout can be daunting, especially when\\nconstrained by space limitations and design principles [6].\\nAs urban housing demands increase and digital tools become\\nmore accessible, there is a growing need for intelligent systems'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='more accessible, there is a growing need for intelligent systems\\nthat simplify architectural planning. Automating the design\\nprocess not only empowers individuals to participate in archi-\\ntectural decisions but also accelerates early-stage prototyping\\nfor professionals [1], [3].\\nDesigning residential floor plans traditionally requires ex-\\ntensive manual effort and specialized architectural knowledge.\\nThis process often presents a barrier for individuals who lack'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='This process often presents a barrier for individuals who lack\\ntechnical skills but wish to customize their living spaces.\\nThe recent advancements in deep learning and graph-based\\nneural networks have enabled new methodologies to automate\\nsuch tasks, allowing for efficient and intelligent floor plan\\ngeneration [5].\\nConventional floor plan design approaches rely heavily on\\nmanual drafting, rule-based procedural modeling, or optimiza-\\ntion algorithms, which are often time-consuming and limited'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='tion algorithms, which are often time-consuming and limited\\nin flexibility [8]. With the evolution of data-driven methods,\\nConvolutional Neural Networks (CNNs) have shown promise\\nin interpreting spatial information , while Graph Neural Net-\\nworks (GNNs) have become effective in modeling complex\\nrelational structures [4]. Combining these techniques allows\\nfor a more comprehensive understanding of spatial layouts,\\nroom adjacencies, and proportional scaling.'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='room adjacencies, and proportional scaling.\\nIn this work, we present an automated system that leverages\\nGNNs, particularly Graph Attention Networks (GATs), for\\ngenerating residential floor plans [10]. The system processes\\nuser-defined inputs, such as plot boundaries and room pref-\\nerences, to output an optimized floor plan. The architecture\\nconsists of two main stages: centroid prediction using a CNN\\nand room size estimation through a GATNet model. The RPlan'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='and room size estimation through a GATNet model. The RPlan\\ndataset, consisting of approximately 80,000 annotated floor\\nplan images, is utilized after conversion into a graph-based\\nformat [2], allowing the model to effectively learn spatial\\nrelationships.\\nOur proposed approach minimizes the complexity for end-\\nusers, reduces reliance on human expertise, and generates lay-\\nouts that are structurally sound and visually coherent. Through\\nextensive evaluation, we demonstrate that the GAT-based sys-'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='extensive evaluation, we demonstrate that the GAT-based sys-\\ntem significantly improves the accuracy and adaptability of\\nfloor plan generation compared to traditional GNN models\\n[7]. This research aims to bridge the gap between artificial\\nintelligence and practical architectural design, offering an\\naccessible tool for automated residential planning.\\nThe remainder of this paper is organized as follows: Section\\nRelated work reviews related work on deep learning for layout'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='Related work reviews related work on deep learning for layout\\ngeneration. Section System architecture presents the designed\\nlayout and includes major stages of layout designing. Section\\nMethodology describes our methodology, including dataset\\npreparation, model architecture, and training strategy. Section\\nExperimental setup and results reports experimental results\\nand evaluations. Finally, Section conclusion concludes the\\npaper and outlines future directions.\\nII. RELATED WORK'),\n",
              " Document(metadata={'page': 0, 'type': 'text'}, page_content='paper and outlines future directions.\\nII. RELATED WORK\\nAutomated floor plan generation has been an active area\\nof research, particularly in leveraging structured representa-\\ntions and deep learning techniques. Hu et al. [1] introduced\\nGraph2Plan, which models spatial layouts using graph struc-\\ntures and learns to generate floor plans by understanding\\nroom connectivity. Paudel et al. [2] proposed a GNN-based\\nclassification method that effectively labels rooms based on'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='graph representations. Jeon et al. [3] built on these ideas with\\nskip-connected layout graphs, enhancing spatial consistency in\\ngenerated outputs. These graph-based methods offer promising\\nalternatives to conventional rule-based procedural approaches.\\nKipf and Welling [4] contributed foundational work on semi-\\nsupervised learning using GCNs, which inspired numerous\\nspatial reasoning models.\\nExtensive surveys such as Wu et al. [5] and Bronstein et al.'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='spatial reasoning models.\\nExtensive surveys such as Wu et al. [5] and Bronstein et al.\\n[6] have demonstrated the strength of graph-based models for\\nnon-Euclidean domains, which include architectural spaces.\\nWang et al. [7] explored dynamic graph representation learning\\nvia self-attention, while Riba et al. [8] offered Kornia to\\naid spatial modeling in PyTorch pipelines. Spectral filtering\\nfor graph convolutions was also proposed by Defferrard et'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='for graph convolutions was also proposed by Defferrard et\\nal. [9], further improving localized learning. Velickovi´c et\\nal. [10] introduced Graph Attention Networks (GATs), which\\nform the basis of our GATNet architecture. Chen et al. [11]\\npresented a graph-based approach for planning real-world\\nbuilding layouts, and Liu et al. [12] utilized sequence models\\nfor translating between layout styles.\\nMoreover, layout-aware methods such as Sun et al. [13],'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='for translating between layout styles.\\nMoreover, layout-aware methods such as Sun et al. [13],\\nYou et al. [14], and Li et al. [15] have advanced spatial\\narrangement learning through deep GNN variants. Generative\\nmethods, including Zhao et al. [16] and Yang et al. [17],\\nexplored object placement and template generation using\\nattention-enhanced GNNs. Symmetry-aware synthesis from\\nZhang et al. [18] and wireframe-based layout generation from'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='Zhang et al. [18] and wireframe-based layout generation from\\nLi et al. [19] further emphasized aesthetic and structural\\nrealism. Lastly, Huang et al. [20] focused on learning layout\\ntemplates from visual floor plan inputs, a concept partially\\naligned with our graph-based encoding of spatial rules.\\nIII. SYSTEM ARCHITECTURE\\nThe proposed system is designed to generate residential\\nfloor plans based on user preferences, integrating deep learning'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='floor plans based on user preferences, integrating deep learning\\nmodels to automate the layout process. It consists of two\\nmajor stages: centroid prediction using a CNN and room size\\nprediction using a GATNet.\\nA. User Input and Preprocessing\\nUsers provide the boundary of their plot and specify prefer-\\nences such as the number of rooms, bathrooms, and kitchens.\\nThis input is parsed to define the expected structure of the\\nfloor plan. Based on this, synthetic graphs are generated,'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='floor plan. Based on this, synthetic graphs are generated,\\nrepresenting rooms as nodes and their relations as edges.\\nB. Centroid Prediction using CNN\\nA pretrained CNN is used to predict the centroids of\\ndifferent room types. These centroids serve as anchor points\\nthat guide the GNN in placing and scaling the rooms within\\nthe available space.\\nC. Graph Generation\\nThe room types and centroid coordinates are transformed\\ninto a graph-based representation. Each node contains fea-'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='into a graph-based representation. Each node contains fea-\\ntures such as room type encoding, estimated area, and initial\\nwidth/height. Edges represent spatial or functional relation-\\nships. The Living-to-All connection pattern is used to ensure\\nevery room is reachable from common areas.\\nD. GATNet for Room Size Estimation\\nA customized GATNet processes the graph to predict room\\ndimensions. Unlike conventional GCNs, the GAT layers apply\\nattention mechanisms, allowing the model to dynamically'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='attention mechanisms, allowing the model to dynamically\\nprioritize relevant neighboring nodes during message passing.\\nE. Layout Rendering\\nPredicted widths and heights from the GATNet are used to\\nrender a 2D layout using matplotlib. Rooms are positioned\\nin a grid-like format, with spacing adjustments to avoid\\noverlaps. Doors and boundaries are added to enhance realism.\\nFig. 1. System Architecture for Floor Plan Generation.\\nIV. METHODOLOGY'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='Fig. 1. System Architecture for Floor Plan Generation.\\nIV. METHODOLOGY\\nThis section details the process involved in designing and\\nimplementing the proposed system for residential floor plan\\ngeneration using deep learning. Our approach is centered\\naround Graph Neural Networks with a focus on GATs, inte-\\ngrated with CNNs for spatial guidance. The pipeline involves\\ndataset conversion, graph construction, model architecture, and\\nprediction-based layout generation.'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='dataset conversion, graph construction, model architecture, and\\nprediction-based layout generation.\\nWe began by utilizing the RPlan dataset, a large-scale\\nfloor plan dataset comprising over 80,000 raster images an-\\nnotated with room labels and boundaries. Since these were\\nnot directly usable for graph-based learning, a preprocessing\\nstep converted each image into a vector-based representation\\nusing geometric processing. By leveraging the Shapely library,'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='using geometric processing. By leveraging the Shapely library,\\nroom contours were extracted and transformed into polygonal\\nstructures, providing spatial metadata for each room.\\nOnce the spatial features were vectorized, we constructed\\ngraph-based representations for each floor plan. Here, each\\nnode represents a room, enriched with features such as room\\ntype encoding, centroid position, bounding box size, and\\nrelative area. Instead of using real adjacency edges, we adopted'),\n",
              " Document(metadata={'page': 1, 'type': 'text'}, page_content='relative area. Instead of using real adjacency edges, we adopted\\na Living-to-All connection pattern where the living room acts\\nas a hub connected to all other rooms. This design ensures\\nthat the model focuses its attention on the central communal'),\n",
              " Document(metadata={'page': 1, 'type': 'image', 'image_id': 'page_1_img_0'}, page_content='[Image: page_1_img_0]'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='Fig. 2. Graph construction using Living-to-All connection strategy, enhancing\\nthe GATNet’s attention on spatial features.\\nspace when propagating information across the graph. Figure 2\\ndemonstrates this graph construction technique.\\nAt the heart of our system is GATNet, a GAT-based\\narchitecture designed for room size prediction. This model\\nprocesses the room graph and a corresponding boundary\\ngraph to learn how room types and their spatial constraints'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='graph to learn how room types and their spatial constraints\\ninteract. Each graph passes through stacked GAT layers with\\nmulti-head attention, allowing the model to weigh different\\nneighbor influences effectively. To mitigate over-smoothing,\\nwe integrated residual connections via feature concatenation\\nrather than skip connections. After extracting deep graph\\nembeddings, the features are merged and passed through\\nadditional GAT layers followed by two MLP branches for'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='additional GAT layers followed by two MLP branches for\\npredicting width and height independently. This decoupled\\nprediction pathway enables the model to fine-tune each spatial\\ndimension with higher precision. The complete architecture is\\nshown in Figure 3.\\nFig. 3. Architecture of GATNet model for room size estimation.\\nAfter training, the model’s predictions were visually vali-\\ndated. Figure 4 shows one such predicted room size output\\nafter processing through the GATNet, providing intuitive con-'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='after processing through the GATNet, providing intuitive con-\\nfirmation of the attention-based dimensional learning.\\nThe model was trained using Mean Squared Error (MSE)\\nloss with the Adam optimizer, a learning rate of 0.001, and a\\nweight decay of 3 × 10−5. It was trained for 300 epochs on\\nFig. 4. GATNet-predicted room sizes from graph-structured floor plan.\\nan Nvidia P100 GPU, demonstrating stable convergence and\\ngeneralization across test examples.'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='an Nvidia P100 GPU, demonstrating stable convergence and\\ngeneralization across test examples.\\nOnce the dimensions were predicted, a spatial layout was\\nconstructed using a grid-based placement strategy. Rooms\\nwere positioned row-wise with configurable gaps to avoid\\noverlaps and maintain realism. The rendering was handled\\nusing the matplotlib library, which allowed for overlaying\\nroom names, door placements, and dimension annotations.\\nTo further refine the output, a post-processing step ensured'),\n",
              " Document(metadata={'page': 2, 'type': 'text'}, page_content='To further refine the output, a post-processing step ensured\\nrooms adhered to the boundary constraints and preserved ad-\\njacency patterns. Doors were strategically placed at midpoints\\nof room edges, and a dashed border was used to represent\\nthe plot boundary. The final rendered layout, as shown in\\nFigure 5, demonstrates the structural and visual coherence\\nachieved through this pipeline.\\nFig. 5. Final floor plan layout predicted by GATNet based on user-defined\\nconstraints.'),\n",
              " Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_0'}, page_content='[Image: page_2_img_0]'),\n",
              " Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_1'}, page_content='[Image: page_2_img_1]'),\n",
              " Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_2'}, page_content='[Image: page_2_img_2]'),\n",
              " Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_3'}, page_content='[Image: page_2_img_3]'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='V. EXPERIMENTAL SETUP AND EVALUATION\\nTo evaluate the performance of the proposed GATNet model\\nfor automated floor plan generation, the system was imple-\\nmented using Python 3.9, with core dependencies including\\nPyTorch and PyTorch Geometric for model training and infer-\\nence. Architectural modeling and visualization were supported\\nusing NetworkX and Matplotlib libraries. All experiments\\nwere conducted on a machine equipped with an NVIDIA Tesla'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='were conducted on a machine equipped with an NVIDIA Tesla\\nP100 GPU and 16GB RAM, ensuring efficient training for\\ndeep models. The dataset used is the RPlan dataset, comprising\\napproximately 80,000 floor plan images. The images were\\nconverted into graph-based representations through a series\\nof preprocessing steps involving polygon-to-node transforma-\\ntions and synthetic edge construction using the Living-to-All\\nconnection strategy. For the purpose of model evaluation, an'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='connection strategy. For the purpose of model evaluation, an\\n80-20 train-test split was employed.\\nThe model was optimized using the Adam optimizer with a\\nlearning rate of 0.001 and a weight decay factor of 3 × 10−5.\\nA learning rate scheduler with a decay gamma of 0.95 was\\napplied to stabilize training. The model was trained for 300\\nepochs using MSE as the loss function to minimize the gap\\nbetween predicted and ground-truth room dimensions. This ex-'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='between predicted and ground-truth room dimensions. This ex-\\nperimental configuration allowed for end-to-end training, lay-\\nout prediction, and evaluation through a streamlined pipeline.\\nQuantitatively, the model’s performance was evaluated using\\nmetrics such as MSE, MAE, and IoU. GATNet achieved a final\\ntraining loss of 0.58 and a validation loss of 0.78, showing\\nstrong generalization. Furthermore, the model recorded an\\nMAE of 0.89 and an average IoU score of 0.81, reflecting'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='MAE of 0.89 and an average IoU score of 0.81, reflecting\\nprecise alignment between predicted and true room layouts.\\nTo benchmark GATNet against baseline models, a compar-\\native study was conducted using a traditional GCN. As shown\\nin Table I, GATNet outperformed GCN across all evaluation\\nmetrics. The architecture’s use of attention mechanisms and\\nmulti-head edge processing significantly enhanced its learning\\ncapacity and resistance to over-smoothing, resulting in faster'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='capacity and resistance to over-smoothing, resulting in faster\\nconvergence and better spatial accuracy.Here we go for the\\ncomparison of our GATNet model with the baseline model\\nGCN comprising the measures like Accuracy, MAE, MSE,\\nIOU, Convergence Epochs and Over smoothing.\\nTABLE I\\nCOMPARISON BETWEEN GCN AND GATNET MODELS\\nMetric\\nGCN\\nGATNet\\nAccuracy\\n78%\\n94%\\nMAE\\n1.62\\n0.89\\nMSE\\n2.98\\n1.24\\nIoU\\n0.62\\n0.81\\nConvergence Epochs\\n110\\n68\\nOver-smoothing\\nPresent\\nMitigated via Attention\\nEdge Feature Utiliza-\\ntion'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='Convergence Epochs\\n110\\n68\\nOver-smoothing\\nPresent\\nMitigated via Attention\\nEdge Feature Utiliza-\\ntion\\nLimited\\nMulti-Head Attention\\nFigure 5: Training vs Validation Loss Curve for GATNet\\nModel.\\nIn terms of visual quality, the GATNet model generated\\nstructurally coherent layouts aligned with user-defined con-\\nstraints. These layouts included labeled room segments, cor-\\nrect spatial adjacency, and consistent scaling. Figure 6 shows\\na final output from the GATNet pipeline.'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='a final output from the GATNet pipeline.\\nFigure 6: Final generated floor plan using the proposed\\nGATNet model.\\nTo further enhance accessibility, the system was deployed\\nusing the Streamlit framework. A user-friendly UI allows users\\nto input their boundary shape and room preferences, run model\\ninference, and visualize the resulting floor plan layout in real-\\ntime. This visual feedback loop makes the platform suitable\\nfor both novice and expert users. Figure 7 presents a snapshot'),\n",
              " Document(metadata={'page': 3, 'type': 'text'}, page_content='for both novice and expert users. Figure 7 presents a snapshot\\nof the interface.'),\n",
              " Document(metadata={'page': 3, 'type': 'image', 'image_id': 'page_3_img_0'}, page_content='[Image: page_3_img_0]'),\n",
              " Document(metadata={'page': 3, 'type': 'image', 'image_id': 'page_3_img_1'}, page_content='[Image: page_3_img_1]'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='Figure 7: Web-based interface built using Streamlit for real-\\ntime layout visualization.\\nVI. CONCLUSION\\nIn this research, we proposed a novel deep learning-based\\nframework for residential floor plan generation using GNNs\\nand GATs. By converting traditional floor plan images into\\nstructured graph representations, our system effectively cap-\\ntured spatial relationships between rooms. The customized\\nGATNet model addressed the over-smoothing issue through'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='GATNet model addressed the over-smoothing issue through\\nconcatenation-based residual connections and multi-head at-\\ntention, resulting in improved learning capacity. Evaluations\\nacross multiple metrics—including Room Type Accuracy,\\nMAE, MSE, and IoU—demonstrated that GATNet signifi-\\ncantly outperformed traditional GCN architectures. Addition-\\nally, we developed an interactive application that enables\\nusers to input design preferences and receive optimized, auto-'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='users to input design preferences and receive optimized, auto-\\ngenerated floor layouts, helping democratize architectural de-\\nsign for non-experts.\\nLooking forward, the framework can be further enhanced\\nby extending it to support 3D layout generation and multi-\\nstory residential structures, including considerations for verti-\\ncal circulation. Moreover, incorporating real-world constraints\\nsuch as sunlight orientation, ventilation requirements, and'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='such as sunlight orientation, ventilation requirements, and\\nbuilding regulations could further tailor generated layouts to\\npractical needs. These directions hold promise for evolving the\\nsystem into a robust and intelligent design assistant for both\\nprofessional architects and end users. .\\nREFERENCES\\n[1] R. Hu, L. Jiang, M. Yang, et al., “Graph2Plan: Learning Floorplan\\nGeneration from Layout Graphs,” arXiv preprint arXiv:2004.13204,\\n2020.'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='Generation from Layout Graphs,” arXiv preprint arXiv:2004.13204,\\n2020.\\n[2] D. P. Paudel, D. Adiguna, A. Habed, et al., “Room Classification\\non Floor Plan Graphs Using Graph Neural Networks,” arXiv preprint\\narXiv:2108.05947, 2021.\\n[3] J. Jeon, S. Park, J. Kim, et al., “Skip-Connected Neural Networks\\nwith Layout Graphs for Floor Plan Auto-Generation,” arXiv preprint\\narXiv:2309.13881, 2023.\\n[4] T. N. Kipf and M. Welling, “Semi-Supervised Classification with Graph'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='arXiv:2309.13881, 2023.\\n[4] T. N. Kipf and M. Welling, “Semi-Supervised Classification with Graph\\nConvolutional Networks,” arXiv preprint arXiv:1609.02907, 2017.\\n[5] Z. Wu, S. Pan, F. Chen, et al., “A Comprehensive Survey on Graph\\nNeural Networks,” IEEE Transactions on Neural Networks and Learning\\nSystems, 2020.\\n[6] M. M. Bronstein, J. Bruna, Y. LeCun, et al., “Geometric Deep Learning:\\nGoing beyond Euclidean Data,” IEEE Signal Processing Magazine,\\n2017.'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='Going beyond Euclidean Data,” IEEE Signal Processing Magazine,\\n2017.\\n[7] W. Wang, Y. Wu, S. Bai, et al., “Dynamic Graph Representation\\nLearning via Self-Attention Networks,” IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, 2021.\\n[8] E. Riba, D. Mishkin, D. Ponsa, et al., “Kornia: An Open-Source\\nDifferentiable Computer Vision Library for PyTorch,” Proc. IEEE/CVF\\nWinter Conference on Applications of Computer Vision, 2020.'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='Winter Conference on Applications of Computer Vision, 2020.\\n[9] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional Neural\\nNetworks on Graphs with Fast Localized Spectral Filtering,” NeurIPS,\\n2016.\\n[10] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and\\nY. Bengio, “Graph Attention Networks,” International Conference on\\nLearning Representations (ICLR), 2018.\\n[11] J. Chen, T. Ma, and C. Xiong, “Graph-based Planning and Generation of'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='[11] J. Chen, T. Ma, and C. Xiong, “Graph-based Planning and Generation of\\nBuilding Floor Plans,” Proceedings of the AAAI Conference on Artificial\\nIntelligence, 2021.\\n[12] Y. Liu, W. Kang, et al., “LayoutTrans: Learning Floor Plan Translation\\nvia Sequence-to-Sequence Modeling,” IEEE Transactions on Multime-\\ndia, 2022.\\n[13] M. Sun, Y. Chen, et al., “Geometry-Aware Graph Neural Network for\\nLayout Generation,” ACM Transactions on Graphics, 2021.'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='Layout Generation,” ACM Transactions on Graphics, 2021.\\n[14] J. You, R. Ying, et al., “Graph Structure Learning for Robust Graph\\nNeural Networks,” Proceedings of ICML, 2020.\\n[15] G. Li, M. M¨uller, et al., “DeepGCNs: Can GCNs Go as Deep as CNNs?,”\\nICCV, 2020.\\n[16] B. Zhao, M. Xu, et al., “Object Layout Generation from Text Description\\nwith Edge-enhanced Graph Attention Network,” ECCV, 2020.\\n[17] W. Yang, X. Tang, and Y. Lin, “Deep Generative Modeling for Layout'),\n",
              " Document(metadata={'page': 4, 'type': 'text'}, page_content='[17] W. Yang, X. Tang, and Y. Lin, “Deep Generative Modeling for Layout\\nSynthesis from Examples,” ACM Transactions on Graphics, 2019.\\n[18] Z. Zhang, et al., “Symmetry-Aware Layout Generation for Floor Plans,”\\nACM Transactions on Graphics, 2020.\\n[19] J. Li, H. Zhang, et al., “LayoutGAN: Generating Graphic Layouts with\\nWireframe Discriminator,” ICLR, 2019.\\n[20] Q. Huang, Y. Zhou, et al., “Learning to Predict Layout Templates from\\nFloorplan Images,” CVPR, 2019.'),\n",
              " Document(metadata={'page': 4, 'type': 'image', 'image_id': 'page_4_img_0'}, page_content='[Image: page_4_img_0]')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create unified FAISS vector store with CLIP embeddings\n",
        "embeddings_array = np.array(all_embeddings)\n",
        "embeddings_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q_sTfMJFdmW",
        "outputId": "3be21efb-bc28-498f-8ee6-5e3bca8e04ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.02613945,  0.00923622,  0.00371121, ...,  0.04285223,\n",
              "         0.01664223, -0.0820921 ],\n",
              "       [ 0.00690663, -0.01337911,  0.00564018, ..., -0.0034973 ,\n",
              "        -0.00344081, -0.08734106],\n",
              "       [ 0.00687354,  0.06079073,  0.02066886, ..., -0.0505387 ,\n",
              "        -0.02636923, -0.02971604],\n",
              "       ...,\n",
              "       [ 0.04825879, -0.00612409, -0.06577697, ..., -0.08738431,\n",
              "        -0.04214821,  0.01525809],\n",
              "       [ 0.02978467, -0.02213261, -0.06427867, ..., -0.05599627,\n",
              "        -0.05051329, -0.01867991],\n",
              "       [ 0.00640715,  0.00714561,  0.00942501, ...,  0.12888741,\n",
              "        -0.00629469,  0.03791992]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(all_docs,embeddings_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igWy6ubwFiuK",
        "outputId": "0809cca6-56c5-4d92-8bec-82e2b829e6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Document(metadata={'page': 0, 'type': 'text'}, page_content='Residential Floor Plan Generation Using Graph\\nNeural Networks (GNNs)\\nAnkur Jain, Nikhil Kumar Singh, Kale Vishwajit Shankarrao, Dr Nilesh Bhosle\\nDepartment of AIML,NIMS Institute of Engineering Technology, Jaipur, India\\nEmail: jankur168@gmail.com\\nAbstract—This paper presents an automated residential floor\\nplan generation system using deep learning techniques, partic-\\nularly Graph Neural Networks and Graph Attention Networks.\\nTraditional floor plan creation is a manual, time-intensive process'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='Traditional floor plan creation is a manual, time-intensive process\\nrequiring architectural expertise, making it less accessible to\\nnon-experts. The proposed approach streamlines this process\\nby converting user-defined spatial requirements into structured\\ngraph representations where rooms are treated as nodes and\\nspatial connections as edges. Initially, a Convolutional Neural\\nNetwork predicts room centroids based on user inputs and\\nboundary maps. These centroid-level features are then processed'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='boundary maps. These centroid-level features are then processed\\nby a custom attention-based model, which refines room sizes\\nby capturing inter-room dependencies and enhancing layout\\ncoherence. The model is trained on the RPlan dataset, which\\nhas been converted into a geometric and graph-based format. A\\nkey strength of this work is its integration with a user-friendly\\nStreamlit-based interface, enabling real-time floor plan visualiza-'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='Streamlit-based interface, enabling real-time floor plan visualiza-\\ntion. Experimental results demonstrate substantial improvements\\nover baseline methods, validating the effectiveness of our deep\\nlearning pipeline for residential architectural design automation.\\nIndex Terms—Graph Neural Networks, Floor Plan Genera-\\ntion,Graph Attention Networks , RPlan Dataset, Convolutional\\nNeural Network , Residential architectural design automation.\\nI. INTRODUCTION'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='Neural Network , Residential architectural design automation.\\nI. INTRODUCTION\\nThe dream of designing personalized residential spaces is\\noften hindered by the technical expertise required in archi-\\ntectural drafting. For non-experts, visualizing and creating\\na functional home layout can be daunting, especially when\\nconstrained by space limitations and design principles [6].\\nAs urban housing demands increase and digital tools become\\nmore accessible, there is a growing need for intelligent systems'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='more accessible, there is a growing need for intelligent systems\\nthat simplify architectural planning. Automating the design\\nprocess not only empowers individuals to participate in archi-\\ntectural decisions but also accelerates early-stage prototyping\\nfor professionals [1], [3].\\nDesigning residential floor plans traditionally requires ex-\\ntensive manual effort and specialized architectural knowledge.\\nThis process often presents a barrier for individuals who lack'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='This process often presents a barrier for individuals who lack\\ntechnical skills but wish to customize their living spaces.\\nThe recent advancements in deep learning and graph-based\\nneural networks have enabled new methodologies to automate\\nsuch tasks, allowing for efficient and intelligent floor plan\\ngeneration [5].\\nConventional floor plan design approaches rely heavily on\\nmanual drafting, rule-based procedural modeling, or optimiza-\\ntion algorithms, which are often time-consuming and limited'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='tion algorithms, which are often time-consuming and limited\\nin flexibility [8]. With the evolution of data-driven methods,\\nConvolutional Neural Networks (CNNs) have shown promise\\nin interpreting spatial information , while Graph Neural Net-\\nworks (GNNs) have become effective in modeling complex\\nrelational structures [4]. Combining these techniques allows\\nfor a more comprehensive understanding of spatial layouts,\\nroom adjacencies, and proportional scaling.'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='room adjacencies, and proportional scaling.\\nIn this work, we present an automated system that leverages\\nGNNs, particularly Graph Attention Networks (GATs), for\\ngenerating residential floor plans [10]. The system processes\\nuser-defined inputs, such as plot boundaries and room pref-\\nerences, to output an optimized floor plan. The architecture\\nconsists of two main stages: centroid prediction using a CNN\\nand room size estimation through a GATNet model. The RPlan'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='and room size estimation through a GATNet model. The RPlan\\ndataset, consisting of approximately 80,000 annotated floor\\nplan images, is utilized after conversion into a graph-based\\nformat [2], allowing the model to effectively learn spatial\\nrelationships.\\nOur proposed approach minimizes the complexity for end-\\nusers, reduces reliance on human expertise, and generates lay-\\nouts that are structurally sound and visually coherent. Through\\nextensive evaluation, we demonstrate that the GAT-based sys-'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='extensive evaluation, we demonstrate that the GAT-based sys-\\ntem significantly improves the accuracy and adaptability of\\nfloor plan generation compared to traditional GNN models\\n[7]. This research aims to bridge the gap between artificial\\nintelligence and practical architectural design, offering an\\naccessible tool for automated residential planning.\\nThe remainder of this paper is organized as follows: Section\\nRelated work reviews related work on deep learning for layout'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='Related work reviews related work on deep learning for layout\\ngeneration. Section System architecture presents the designed\\nlayout and includes major stages of layout designing. Section\\nMethodology describes our methodology, including dataset\\npreparation, model architecture, and training strategy. Section\\nExperimental setup and results reports experimental results\\nand evaluations. Finally, Section conclusion concludes the\\npaper and outlines future directions.\\nII. RELATED WORK'),\n",
              "  Document(metadata={'page': 0, 'type': 'text'}, page_content='paper and outlines future directions.\\nII. RELATED WORK\\nAutomated floor plan generation has been an active area\\nof research, particularly in leveraging structured representa-\\ntions and deep learning techniques. Hu et al. [1] introduced\\nGraph2Plan, which models spatial layouts using graph struc-\\ntures and learns to generate floor plans by understanding\\nroom connectivity. Paudel et al. [2] proposed a GNN-based\\nclassification method that effectively labels rooms based on'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='graph representations. Jeon et al. [3] built on these ideas with\\nskip-connected layout graphs, enhancing spatial consistency in\\ngenerated outputs. These graph-based methods offer promising\\nalternatives to conventional rule-based procedural approaches.\\nKipf and Welling [4] contributed foundational work on semi-\\nsupervised learning using GCNs, which inspired numerous\\nspatial reasoning models.\\nExtensive surveys such as Wu et al. [5] and Bronstein et al.'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='spatial reasoning models.\\nExtensive surveys such as Wu et al. [5] and Bronstein et al.\\n[6] have demonstrated the strength of graph-based models for\\nnon-Euclidean domains, which include architectural spaces.\\nWang et al. [7] explored dynamic graph representation learning\\nvia self-attention, while Riba et al. [8] offered Kornia to\\naid spatial modeling in PyTorch pipelines. Spectral filtering\\nfor graph convolutions was also proposed by Defferrard et'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='for graph convolutions was also proposed by Defferrard et\\nal. [9], further improving localized learning. Velickovi´c et\\nal. [10] introduced Graph Attention Networks (GATs), which\\nform the basis of our GATNet architecture. Chen et al. [11]\\npresented a graph-based approach for planning real-world\\nbuilding layouts, and Liu et al. [12] utilized sequence models\\nfor translating between layout styles.\\nMoreover, layout-aware methods such as Sun et al. [13],'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='for translating between layout styles.\\nMoreover, layout-aware methods such as Sun et al. [13],\\nYou et al. [14], and Li et al. [15] have advanced spatial\\narrangement learning through deep GNN variants. Generative\\nmethods, including Zhao et al. [16] and Yang et al. [17],\\nexplored object placement and template generation using\\nattention-enhanced GNNs. Symmetry-aware synthesis from\\nZhang et al. [18] and wireframe-based layout generation from'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='Zhang et al. [18] and wireframe-based layout generation from\\nLi et al. [19] further emphasized aesthetic and structural\\nrealism. Lastly, Huang et al. [20] focused on learning layout\\ntemplates from visual floor plan inputs, a concept partially\\naligned with our graph-based encoding of spatial rules.\\nIII. SYSTEM ARCHITECTURE\\nThe proposed system is designed to generate residential\\nfloor plans based on user preferences, integrating deep learning'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='floor plans based on user preferences, integrating deep learning\\nmodels to automate the layout process. It consists of two\\nmajor stages: centroid prediction using a CNN and room size\\nprediction using a GATNet.\\nA. User Input and Preprocessing\\nUsers provide the boundary of their plot and specify prefer-\\nences such as the number of rooms, bathrooms, and kitchens.\\nThis input is parsed to define the expected structure of the\\nfloor plan. Based on this, synthetic graphs are generated,'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='floor plan. Based on this, synthetic graphs are generated,\\nrepresenting rooms as nodes and their relations as edges.\\nB. Centroid Prediction using CNN\\nA pretrained CNN is used to predict the centroids of\\ndifferent room types. These centroids serve as anchor points\\nthat guide the GNN in placing and scaling the rooms within\\nthe available space.\\nC. Graph Generation\\nThe room types and centroid coordinates are transformed\\ninto a graph-based representation. Each node contains fea-'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='into a graph-based representation. Each node contains fea-\\ntures such as room type encoding, estimated area, and initial\\nwidth/height. Edges represent spatial or functional relation-\\nships. The Living-to-All connection pattern is used to ensure\\nevery room is reachable from common areas.\\nD. GATNet for Room Size Estimation\\nA customized GATNet processes the graph to predict room\\ndimensions. Unlike conventional GCNs, the GAT layers apply\\nattention mechanisms, allowing the model to dynamically'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='attention mechanisms, allowing the model to dynamically\\nprioritize relevant neighboring nodes during message passing.\\nE. Layout Rendering\\nPredicted widths and heights from the GATNet are used to\\nrender a 2D layout using matplotlib. Rooms are positioned\\nin a grid-like format, with spacing adjustments to avoid\\noverlaps. Doors and boundaries are added to enhance realism.\\nFig. 1. System Architecture for Floor Plan Generation.\\nIV. METHODOLOGY'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='Fig. 1. System Architecture for Floor Plan Generation.\\nIV. METHODOLOGY\\nThis section details the process involved in designing and\\nimplementing the proposed system for residential floor plan\\ngeneration using deep learning. Our approach is centered\\naround Graph Neural Networks with a focus on GATs, inte-\\ngrated with CNNs for spatial guidance. The pipeline involves\\ndataset conversion, graph construction, model architecture, and\\nprediction-based layout generation.'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='dataset conversion, graph construction, model architecture, and\\nprediction-based layout generation.\\nWe began by utilizing the RPlan dataset, a large-scale\\nfloor plan dataset comprising over 80,000 raster images an-\\nnotated with room labels and boundaries. Since these were\\nnot directly usable for graph-based learning, a preprocessing\\nstep converted each image into a vector-based representation\\nusing geometric processing. By leveraging the Shapely library,'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='using geometric processing. By leveraging the Shapely library,\\nroom contours were extracted and transformed into polygonal\\nstructures, providing spatial metadata for each room.\\nOnce the spatial features were vectorized, we constructed\\ngraph-based representations for each floor plan. Here, each\\nnode represents a room, enriched with features such as room\\ntype encoding, centroid position, bounding box size, and\\nrelative area. Instead of using real adjacency edges, we adopted'),\n",
              "  Document(metadata={'page': 1, 'type': 'text'}, page_content='relative area. Instead of using real adjacency edges, we adopted\\na Living-to-All connection pattern where the living room acts\\nas a hub connected to all other rooms. This design ensures\\nthat the model focuses its attention on the central communal'),\n",
              "  Document(metadata={'page': 1, 'type': 'image', 'image_id': 'page_1_img_0'}, page_content='[Image: page_1_img_0]'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='Fig. 2. Graph construction using Living-to-All connection strategy, enhancing\\nthe GATNet’s attention on spatial features.\\nspace when propagating information across the graph. Figure 2\\ndemonstrates this graph construction technique.\\nAt the heart of our system is GATNet, a GAT-based\\narchitecture designed for room size prediction. This model\\nprocesses the room graph and a corresponding boundary\\ngraph to learn how room types and their spatial constraints'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='graph to learn how room types and their spatial constraints\\ninteract. Each graph passes through stacked GAT layers with\\nmulti-head attention, allowing the model to weigh different\\nneighbor influences effectively. To mitigate over-smoothing,\\nwe integrated residual connections via feature concatenation\\nrather than skip connections. After extracting deep graph\\nembeddings, the features are merged and passed through\\nadditional GAT layers followed by two MLP branches for'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='additional GAT layers followed by two MLP branches for\\npredicting width and height independently. This decoupled\\nprediction pathway enables the model to fine-tune each spatial\\ndimension with higher precision. The complete architecture is\\nshown in Figure 3.\\nFig. 3. Architecture of GATNet model for room size estimation.\\nAfter training, the model’s predictions were visually vali-\\ndated. Figure 4 shows one such predicted room size output\\nafter processing through the GATNet, providing intuitive con-'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='after processing through the GATNet, providing intuitive con-\\nfirmation of the attention-based dimensional learning.\\nThe model was trained using Mean Squared Error (MSE)\\nloss with the Adam optimizer, a learning rate of 0.001, and a\\nweight decay of 3 × 10−5. It was trained for 300 epochs on\\nFig. 4. GATNet-predicted room sizes from graph-structured floor plan.\\nan Nvidia P100 GPU, demonstrating stable convergence and\\ngeneralization across test examples.'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='an Nvidia P100 GPU, demonstrating stable convergence and\\ngeneralization across test examples.\\nOnce the dimensions were predicted, a spatial layout was\\nconstructed using a grid-based placement strategy. Rooms\\nwere positioned row-wise with configurable gaps to avoid\\noverlaps and maintain realism. The rendering was handled\\nusing the matplotlib library, which allowed for overlaying\\nroom names, door placements, and dimension annotations.\\nTo further refine the output, a post-processing step ensured'),\n",
              "  Document(metadata={'page': 2, 'type': 'text'}, page_content='To further refine the output, a post-processing step ensured\\nrooms adhered to the boundary constraints and preserved ad-\\njacency patterns. Doors were strategically placed at midpoints\\nof room edges, and a dashed border was used to represent\\nthe plot boundary. The final rendered layout, as shown in\\nFigure 5, demonstrates the structural and visual coherence\\nachieved through this pipeline.\\nFig. 5. Final floor plan layout predicted by GATNet based on user-defined\\nconstraints.'),\n",
              "  Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_0'}, page_content='[Image: page_2_img_0]'),\n",
              "  Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_1'}, page_content='[Image: page_2_img_1]'),\n",
              "  Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_2'}, page_content='[Image: page_2_img_2]'),\n",
              "  Document(metadata={'page': 2, 'type': 'image', 'image_id': 'page_2_img_3'}, page_content='[Image: page_2_img_3]'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='V. EXPERIMENTAL SETUP AND EVALUATION\\nTo evaluate the performance of the proposed GATNet model\\nfor automated floor plan generation, the system was imple-\\nmented using Python 3.9, with core dependencies including\\nPyTorch and PyTorch Geometric for model training and infer-\\nence. Architectural modeling and visualization were supported\\nusing NetworkX and Matplotlib libraries. All experiments\\nwere conducted on a machine equipped with an NVIDIA Tesla'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='were conducted on a machine equipped with an NVIDIA Tesla\\nP100 GPU and 16GB RAM, ensuring efficient training for\\ndeep models. The dataset used is the RPlan dataset, comprising\\napproximately 80,000 floor plan images. The images were\\nconverted into graph-based representations through a series\\nof preprocessing steps involving polygon-to-node transforma-\\ntions and synthetic edge construction using the Living-to-All\\nconnection strategy. For the purpose of model evaluation, an'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='connection strategy. For the purpose of model evaluation, an\\n80-20 train-test split was employed.\\nThe model was optimized using the Adam optimizer with a\\nlearning rate of 0.001 and a weight decay factor of 3 × 10−5.\\nA learning rate scheduler with a decay gamma of 0.95 was\\napplied to stabilize training. The model was trained for 300\\nepochs using MSE as the loss function to minimize the gap\\nbetween predicted and ground-truth room dimensions. This ex-'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='between predicted and ground-truth room dimensions. This ex-\\nperimental configuration allowed for end-to-end training, lay-\\nout prediction, and evaluation through a streamlined pipeline.\\nQuantitatively, the model’s performance was evaluated using\\nmetrics such as MSE, MAE, and IoU. GATNet achieved a final\\ntraining loss of 0.58 and a validation loss of 0.78, showing\\nstrong generalization. Furthermore, the model recorded an\\nMAE of 0.89 and an average IoU score of 0.81, reflecting'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='MAE of 0.89 and an average IoU score of 0.81, reflecting\\nprecise alignment between predicted and true room layouts.\\nTo benchmark GATNet against baseline models, a compar-\\native study was conducted using a traditional GCN. As shown\\nin Table I, GATNet outperformed GCN across all evaluation\\nmetrics. The architecture’s use of attention mechanisms and\\nmulti-head edge processing significantly enhanced its learning\\ncapacity and resistance to over-smoothing, resulting in faster'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='capacity and resistance to over-smoothing, resulting in faster\\nconvergence and better spatial accuracy.Here we go for the\\ncomparison of our GATNet model with the baseline model\\nGCN comprising the measures like Accuracy, MAE, MSE,\\nIOU, Convergence Epochs and Over smoothing.\\nTABLE I\\nCOMPARISON BETWEEN GCN AND GATNET MODELS\\nMetric\\nGCN\\nGATNet\\nAccuracy\\n78%\\n94%\\nMAE\\n1.62\\n0.89\\nMSE\\n2.98\\n1.24\\nIoU\\n0.62\\n0.81\\nConvergence Epochs\\n110\\n68\\nOver-smoothing\\nPresent\\nMitigated via Attention\\nEdge Feature Utiliza-\\ntion'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='Convergence Epochs\\n110\\n68\\nOver-smoothing\\nPresent\\nMitigated via Attention\\nEdge Feature Utiliza-\\ntion\\nLimited\\nMulti-Head Attention\\nFigure 5: Training vs Validation Loss Curve for GATNet\\nModel.\\nIn terms of visual quality, the GATNet model generated\\nstructurally coherent layouts aligned with user-defined con-\\nstraints. These layouts included labeled room segments, cor-\\nrect spatial adjacency, and consistent scaling. Figure 6 shows\\na final output from the GATNet pipeline.'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='a final output from the GATNet pipeline.\\nFigure 6: Final generated floor plan using the proposed\\nGATNet model.\\nTo further enhance accessibility, the system was deployed\\nusing the Streamlit framework. A user-friendly UI allows users\\nto input their boundary shape and room preferences, run model\\ninference, and visualize the resulting floor plan layout in real-\\ntime. This visual feedback loop makes the platform suitable\\nfor both novice and expert users. Figure 7 presents a snapshot'),\n",
              "  Document(metadata={'page': 3, 'type': 'text'}, page_content='for both novice and expert users. Figure 7 presents a snapshot\\nof the interface.'),\n",
              "  Document(metadata={'page': 3, 'type': 'image', 'image_id': 'page_3_img_0'}, page_content='[Image: page_3_img_0]'),\n",
              "  Document(metadata={'page': 3, 'type': 'image', 'image_id': 'page_3_img_1'}, page_content='[Image: page_3_img_1]'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='Figure 7: Web-based interface built using Streamlit for real-\\ntime layout visualization.\\nVI. CONCLUSION\\nIn this research, we proposed a novel deep learning-based\\nframework for residential floor plan generation using GNNs\\nand GATs. By converting traditional floor plan images into\\nstructured graph representations, our system effectively cap-\\ntured spatial relationships between rooms. The customized\\nGATNet model addressed the over-smoothing issue through'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='GATNet model addressed the over-smoothing issue through\\nconcatenation-based residual connections and multi-head at-\\ntention, resulting in improved learning capacity. Evaluations\\nacross multiple metrics—including Room Type Accuracy,\\nMAE, MSE, and IoU—demonstrated that GATNet signifi-\\ncantly outperformed traditional GCN architectures. Addition-\\nally, we developed an interactive application that enables\\nusers to input design preferences and receive optimized, auto-'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='users to input design preferences and receive optimized, auto-\\ngenerated floor layouts, helping democratize architectural de-\\nsign for non-experts.\\nLooking forward, the framework can be further enhanced\\nby extending it to support 3D layout generation and multi-\\nstory residential structures, including considerations for verti-\\ncal circulation. Moreover, incorporating real-world constraints\\nsuch as sunlight orientation, ventilation requirements, and'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='such as sunlight orientation, ventilation requirements, and\\nbuilding regulations could further tailor generated layouts to\\npractical needs. These directions hold promise for evolving the\\nsystem into a robust and intelligent design assistant for both\\nprofessional architects and end users. .\\nREFERENCES\\n[1] R. Hu, L. Jiang, M. Yang, et al., “Graph2Plan: Learning Floorplan\\nGeneration from Layout Graphs,” arXiv preprint arXiv:2004.13204,\\n2020.'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='Generation from Layout Graphs,” arXiv preprint arXiv:2004.13204,\\n2020.\\n[2] D. P. Paudel, D. Adiguna, A. Habed, et al., “Room Classification\\non Floor Plan Graphs Using Graph Neural Networks,” arXiv preprint\\narXiv:2108.05947, 2021.\\n[3] J. Jeon, S. Park, J. Kim, et al., “Skip-Connected Neural Networks\\nwith Layout Graphs for Floor Plan Auto-Generation,” arXiv preprint\\narXiv:2309.13881, 2023.\\n[4] T. N. Kipf and M. Welling, “Semi-Supervised Classification with Graph'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='arXiv:2309.13881, 2023.\\n[4] T. N. Kipf and M. Welling, “Semi-Supervised Classification with Graph\\nConvolutional Networks,” arXiv preprint arXiv:1609.02907, 2017.\\n[5] Z. Wu, S. Pan, F. Chen, et al., “A Comprehensive Survey on Graph\\nNeural Networks,” IEEE Transactions on Neural Networks and Learning\\nSystems, 2020.\\n[6] M. M. Bronstein, J. Bruna, Y. LeCun, et al., “Geometric Deep Learning:\\nGoing beyond Euclidean Data,” IEEE Signal Processing Magazine,\\n2017.'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='Going beyond Euclidean Data,” IEEE Signal Processing Magazine,\\n2017.\\n[7] W. Wang, Y. Wu, S. Bai, et al., “Dynamic Graph Representation\\nLearning via Self-Attention Networks,” IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, 2021.\\n[8] E. Riba, D. Mishkin, D. Ponsa, et al., “Kornia: An Open-Source\\nDifferentiable Computer Vision Library for PyTorch,” Proc. IEEE/CVF\\nWinter Conference on Applications of Computer Vision, 2020.'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='Winter Conference on Applications of Computer Vision, 2020.\\n[9] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional Neural\\nNetworks on Graphs with Fast Localized Spectral Filtering,” NeurIPS,\\n2016.\\n[10] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and\\nY. Bengio, “Graph Attention Networks,” International Conference on\\nLearning Representations (ICLR), 2018.\\n[11] J. Chen, T. Ma, and C. Xiong, “Graph-based Planning and Generation of'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='[11] J. Chen, T. Ma, and C. Xiong, “Graph-based Planning and Generation of\\nBuilding Floor Plans,” Proceedings of the AAAI Conference on Artificial\\nIntelligence, 2021.\\n[12] Y. Liu, W. Kang, et al., “LayoutTrans: Learning Floor Plan Translation\\nvia Sequence-to-Sequence Modeling,” IEEE Transactions on Multime-\\ndia, 2022.\\n[13] M. Sun, Y. Chen, et al., “Geometry-Aware Graph Neural Network for\\nLayout Generation,” ACM Transactions on Graphics, 2021.'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='Layout Generation,” ACM Transactions on Graphics, 2021.\\n[14] J. You, R. Ying, et al., “Graph Structure Learning for Robust Graph\\nNeural Networks,” Proceedings of ICML, 2020.\\n[15] G. Li, M. M¨uller, et al., “DeepGCNs: Can GCNs Go as Deep as CNNs?,”\\nICCV, 2020.\\n[16] B. Zhao, M. Xu, et al., “Object Layout Generation from Text Description\\nwith Edge-enhanced Graph Attention Network,” ECCV, 2020.\\n[17] W. Yang, X. Tang, and Y. Lin, “Deep Generative Modeling for Layout'),\n",
              "  Document(metadata={'page': 4, 'type': 'text'}, page_content='[17] W. Yang, X. Tang, and Y. Lin, “Deep Generative Modeling for Layout\\nSynthesis from Examples,” ACM Transactions on Graphics, 2019.\\n[18] Z. Zhang, et al., “Symmetry-Aware Layout Generation for Floor Plans,”\\nACM Transactions on Graphics, 2020.\\n[19] J. Li, H. Zhang, et al., “LayoutGAN: Generating Graphic Layouts with\\nWireframe Discriminator,” ICLR, 2019.\\n[20] Q. Huang, Y. Zhou, et al., “Learning to Predict Layout Templates from\\nFloorplan Images,” CVPR, 2019.'),\n",
              "  Document(metadata={'page': 4, 'type': 'image', 'image_id': 'page_4_img_0'}, page_content='[Image: page_4_img_0]')],\n",
              " array([[ 0.02613945,  0.00923622,  0.00371121, ...,  0.04285223,\n",
              "          0.01664223, -0.0820921 ],\n",
              "        [ 0.00690663, -0.01337911,  0.00564018, ..., -0.0034973 ,\n",
              "         -0.00344081, -0.08734106],\n",
              "        [ 0.00687354,  0.06079073,  0.02066886, ..., -0.0505387 ,\n",
              "         -0.02636923, -0.02971604],\n",
              "        ...,\n",
              "        [ 0.04825879, -0.00612409, -0.06577697, ..., -0.08738431,\n",
              "         -0.04214821,  0.01525809],\n",
              "        [ 0.02978467, -0.02213261, -0.06427867, ..., -0.05599627,\n",
              "         -0.05051329, -0.01867991],\n",
              "        [ 0.00640715,  0.00714561,  0.00942501, ...,  0.12888741,\n",
              "         -0.00629469,  0.03791992]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f49e2750",
        "outputId": "a1f35362-c818-4a41-8e70-6a18729bc9dc"
      },
      "source": [
        "!pip install faiss-cpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom FAISS index since we have precomputed embeddings\n",
        "vector_store = FAISS.from_embeddings(\n",
        "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
        "    embedding=None,  # We're using precomputed embeddings\n",
        "    metadatas=[doc.metadata for doc in all_docs]\n",
        ")\n",
        "vector_store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AemhKomYFpHa",
        "outputId": "8416e02c-3001-4bd6-e29e-fa919d85085a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x7c46c2703950>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GPT-4 Vision model\n",
        "llm = init_chat_model(\"openai:gpt-4.1\")\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VascWGwUF9Q5",
        "outputId": "d54a7f77-6869-4d1a-a320-2e9a5c9c4a97"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7c46c257f990>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7c46c1953190>, root_client=<openai.OpenAI object at 0x7c475eb8de50>, root_async_client=<openai.AsyncOpenAI object at 0x7c46c1957d90>, model_name='gpt-4.1', model_kwargs={}, openai_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-openai"
      ],
      "metadata": {
        "id": "KItVOqGYGAgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a103ac0b-fa40-4da0-8c0b-8ddc98fabcbd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.72)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.97.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMJpdiKRFlhn",
        "outputId": "9ba5d64b-93d3-432c-95d9-06f9981d5ada"
      },
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "# Create the FAISS vector store\n",
        "vector_store = FAISS.from_embeddings(\n",
        "    text_embeddings=list(zip(map(lambda doc: doc.page_content, all_docs), all_embeddings)),\n",
        "    embedding=embed_text,\n",
        "    docstore=InMemoryDocstore({i: doc for i, doc in enumerate(all_docs)}),\n",
        "    index_to_docstore_id={i: i for i in range(len(all_docs))}\n",
        ")\n",
        "vector_store"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x7c46c1952a50>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_multimodal(query, k=5):\n",
        "    \"\"\"Unified retrieval using CLIP embeddings for both text and images.\"\"\"\n",
        "    # Embed query using CLIP\n",
        "    query_embedding = embed_text(query)\n",
        "\n",
        "    # Search in unified vector store\n",
        "    results = vector_store.similarity_search_by_vector(\n",
        "        embedding=query_embedding,\n",
        "        k=k\n",
        "    )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "n2HoJPhxGJEd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multimodal_message(query, retrieved_docs):\n",
        "    \"\"\"Create a message with both text and images for GPT-4V.\"\"\"\n",
        "    content = []\n",
        "\n",
        "    # Add the query\n",
        "    content.append({\n",
        "        \"type\": \"text\",\n",
        "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"\n",
        "    })\n",
        "\n",
        "    # Separate text and image documents\n",
        "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
        "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
        "\n",
        "    # Add text context\n",
        "    if text_docs:\n",
        "        text_context = \"\\n\\n\".join([\n",
        "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
        "            for doc in text_docs\n",
        "        ])\n",
        "        content.append({\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
        "        })\n",
        "\n",
        "    # Add images\n",
        "    for doc in image_docs:\n",
        "        image_id = doc.metadata.get(\"image_id\")\n",
        "        if image_id and image_id in image_data_store:\n",
        "            content.append({\n",
        "                \"type\": \"text\",\n",
        "                \"text\": f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
        "            })\n",
        "            content.append({\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\n",
        "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
        "                }\n",
        "            })\n",
        "\n",
        "    # Add instruction\n",
        "    content.append({\n",
        "        \"type\": \"text\",\n",
        "        \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
        "    })\n",
        "\n",
        "    return HumanMessage(content=content)"
      ],
      "metadata": {
        "id": "0O_P9k4sUmij"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multimodal_pdf_rag_pipeline(query):\n",
        "    \"\"\"Main pipeline for multimodal RAG.\"\"\"\n",
        "    # Retrieve relevant documents\n",
        "    context_docs = retrieve_multimodal(query, k=5)\n",
        "\n",
        "    # Create multimodal message\n",
        "    message = create_multimodal_message(query, context_docs)\n",
        "\n",
        "    # Get response from GPT-4V\n",
        "    response = llm.invoke([message])\n",
        "\n",
        "    # Print retrieved context info\n",
        "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
        "    for doc in context_docs:\n",
        "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
        "        page = doc.metadata.get(\"page\", \"?\")\n",
        "        if doc_type == \"text\":\n",
        "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
        "            print(f\"  - Text from page {page}: {preview}\")\n",
        "        else:\n",
        "            print(f\"  - Image from page {page}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "or_T_UzoUsGl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"What is GATNET Architecture?\",\n",
        "        \"Summarize the main findings from the document\",\n",
        "        \"What visual elements are present in the document?\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        print(\"-\" * 50)\n",
        "        answer = multimodal_pdf_rag_pipeline(query)\n",
        "        print(f\"Answer: {answer}\")\n",
        "        print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uby046-CUwOr",
        "outputId": "0cd497e8-a774-418d-8514-d62dfbcc2f76"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: What is GATNET Architecture?\n",
            "--------------------------------------------------\n",
            "\n",
            "Retrieved 5 documents:\n",
            "  - Text from page 4: GATNet model addressed the over-smoothing issue through\n",
            "concatenation-based residual connections and...\n",
            "  - Text from page 3: for both novice and expert users. Figure 7 presents a snapshot\n",
            "of the interface.\n",
            "  - Text from page 0: more accessible, there is a growing need for intelligent systems\n",
            "that simplify architectural plannin...\n",
            "  - Text from page 1: dataset conversion, graph construction, model architecture, and\n",
            "prediction-based layout generation.\n",
            "...\n",
            "  - Text from page 0: tion algorithms, which are often time-consuming and limited\n",
            "in flexibility [8]. With the evolution o...\n",
            "\n",
            "\n",
            "Answer: **GATNet Architecture** refers to a neural network model designed for tasks like automatic generation and understanding of architectural floor plans, with a particular focus on overcoming the limitations of traditional graph neural networks (GCNs).\n",
            "\n",
            "According to the provided excerpts, here are the key features and context of the GATNet architecture:\n",
            "\n",
            "1. **Core Basis:**\n",
            "   - GATNet is built on Graph Neural Network (GNN) concepts, specifically leveraging *graph attention* mechanisms (as in Graph Attention Networks, or GATs).\n",
            "   - It is designed to capture complex spatial relations and room adjacencies, which are essential for modeling architectural layouts.\n",
            "\n",
            "2. **Model Components:**\n",
            "   - **Multi-head attention:** Allows the model to focus on different structural relationships simultaneously, improving its ability to learn rich representations of floor plan graphs.\n",
            "   - **Concatenation-based residual connections:** These connections address the *over-smoothing* problem common in deep graph networks, helping the network retain meaningful, distinct information across layers.\n",
            "   \n",
            "3. **Workflow:**\n",
            "   - The process begins with a large, annotated floor plan dataset (RPlan), which is converted from raster images to vector-based, graph-like representations suitable for GNN processing.\n",
            "   - Geometric processing (using libraries like Shapely) is used to extract spatial and boundary information of rooms.\n",
            "\n",
            "4. **Performance and Evaluation:**\n",
            "   - GATNet shows improved learning capacity over classical GCNs.\n",
            "   - It significantly outperforms previous models across several metrics (e.g., Room Type Accuracy, MAE, MSE, IoU) when evaluated on tasks like room classification and layout prediction.\n",
            "\n",
            "5. **Application:**\n",
            "   - GATNet powers an interactive application wherein users input design preferences and receive optimized, automated floor plan suggestions—making architectural design more accessible and efficient.\n",
            "\n",
            "**Summary Table**\n",
            "\n",
            "| Feature                   | Description                                                                                           |\n",
            "|---------------------------|-------------------------------------------------------------------------------------------------------|\n",
            "| Input                     | Vectorized graphs of floor plans (rooms as nodes, adjacencies as edges)                               |\n",
            "| Architecture              | Multi-head Attention, Concatenation-based Residual Connections                                        |\n",
            "| Goal                      | Learn complex spatial relationships and generate/understand architectural layouts                     |\n",
            "| Overcomes                 | Over-smoothing in deep GCNs, limited expressivity of traditional CNNs and GCNs                        |\n",
            "| Outcome                   | Higher accuracy and better structured predictions (Rooms, boundaries, adjacencies)                    |\n",
            "\n",
            "**References to context:**\n",
            "- \"GATNet model addressed the over-smoothing issue through concatenation-based residual connections and multi-head attention, resulting in improved learning capacity.\"\n",
            "- \"... demonstrates that GATNet significantly outperformed traditional GCN architectures.\"\n",
            "- Process includes conversion from image to vector/graph for use in GNN-style architectures.\n",
            "\n",
            "---\n",
            "\n",
            "**In summary:**  \n",
            "**GATNet Architecture** is a specialized graph attention neural network that leverages residual connections via concatenation and multi-head attention to more effectively learn and predict room layouts and relationships in architectural floor plans, outperforming earlier graph-based approaches.\n",
            "======================================================================\n",
            "\n",
            "Query: Summarize the main findings from the document\n",
            "--------------------------------------------------\n",
            "\n",
            "Retrieved 5 documents:\n",
            "  - Text from page 3: for both novice and expert users. Figure 7 presents a snapshot\n",
            "of the interface.\n",
            "  - Text from page 3: connection strategy. For the purpose of model evaluation, an\n",
            "80-20 train-test split was employed.\n",
            "Th...\n",
            "  - Text from page 0: more accessible, there is a growing need for intelligent systems\n",
            "that simplify architectural plannin...\n",
            "  - Text from page 1: dataset conversion, graph construction, model architecture, and\n",
            "prediction-based layout generation.\n",
            "...\n",
            "  - Text from page 1: for translating between layout styles.\n",
            "Moreover, layout-aware methods such as Sun et al. [13],\n",
            "You e...\n",
            "\n",
            "\n",
            "Answer: **Main Findings Summary:**\n",
            "\n",
            "- **Motivation & Goal:** There is a growing demand for intelligent systems to simplify architectural planning and automate residential floor plan design, making it accessible to both novices and experts and accelerating the prototyping process for professionals.\n",
            "\n",
            "- **Dataset & Processing:** The study used the large-scale RPlan dataset (>80,000 annotated floor plan images). Since images aren’t directly suitable for graph-based learning, each was converted vectorially via geometric processing with the Shapely library to enable machine learning model input.\n",
            "\n",
            "- **Model & Training:**  \n",
            "  - A machine learning model was developed to predict and generate room layouts.\n",
            "  - The data was split 80-20 for training and testing.\n",
            "  - The Adam optimizer (learning rate = 0.001; weight decay = 3×10⁻⁵) was used; training stabilized with a learning rate scheduler (decay gamma = 0.95).\n",
            "  - Training lasted 300 epochs, minimizing mean squared error (MSE) between predicted and actual room sizes.\n",
            "\n",
            "- **Approach & Novelty:**  \n",
            "  - The approach integrates dataset conversion, graph construction, and deep graph neural network (GNN) architectures, referencing and building upon layout-aware and generative GNN methods for spatial arrangement.\n",
            "  - The generated interface is designed to support both novice and expert interactions.\n",
            "\n",
            "Overall, the work presents a comprehensive pipeline: converting raster floor plans into vectors, employing advanced GNNs for automatic layout generation, and optimizing training for realistic, accurate room prediction, thus lowering the technical barrier for effective architectural floor plan design.\n",
            "======================================================================\n",
            "\n",
            "Query: What visual elements are present in the document?\n",
            "--------------------------------------------------\n",
            "\n",
            "Retrieved 5 documents:\n",
            "  - Text from page 3: for both novice and expert users. Figure 7 presents a snapshot\n",
            "of the interface.\n",
            "  - Text from page 0: more accessible, there is a growing need for intelligent systems\n",
            "that simplify architectural plannin...\n",
            "  - Text from page 1: dataset conversion, graph construction, model architecture, and\n",
            "prediction-based layout generation.\n",
            "...\n",
            "  - Text from page 3: connection strategy. For the purpose of model evaluation, an\n",
            "80-20 train-test split was employed.\n",
            "Th...\n",
            "  - Text from page 3: were conducted on a machine equipped with an NVIDIA Tesla\n",
            "P100 GPU and 16GB RAM, ensuring efficient ...\n",
            "\n",
            "\n",
            "Answer: Based on the provided text excerpts from the document, the following visual elements are referenced:\n",
            "\n",
            "1. **Figure 7 (Page 3):**  \n",
            "   - The text states, “Figure 7 presents a snapshot of the interface.”  \n",
            "   - This implies at least one visual figure is present, specifically showing the user interface of the discussed system, likely related to architectural or floor plan design tools.\n",
            "\n",
            "2. **Raster images of floor plans (Page 1):**  \n",
            "   - The RPlan dataset is described as “comprising over 80,000 raster images annotated with room labels and boundaries.”\n",
            "   - These images are used in the research, and it is plausible that sample images or visual representations of these floorplans with labels/boundaries are shown as visual elements in the document.\n",
            "\n",
            "3. **Graph-based representations / Diagrams (Pages 1, 3):**  \n",
            "   - The process involves converting raster images into “vector-based representation” and “graph-based representations” using polygon-to-node transformations.  \n",
            "   - There may be diagrams or visualizations showing these transformations (e.g., from floor plan image to graph structure).\n",
            "\n",
            "4. **Possible charts or plots (Page 3):**\n",
            "   - Training information (optimizer, learning rate, etc.) and train-test splits are discussed.\n",
            "   - Documents describing machine learning models often include charts or graphs to visualize model performance, learning curves, or dataset statistics.\n",
            "\n",
            "**Summary Table of Visual Elements Present:**\n",
            "\n",
            "| Visual Element               | Description / Context                             |\n",
            "|-----------------------------|---------------------------------------------------|\n",
            "| User Interface Snapshot      | Referenced as Figure 7, likely a UI screenshot   |\n",
            "| Annotated Floor Plan Images  | Mention of labeled floor plan images (RPlan data) |\n",
            "| Graph/Diagram Visualization  | Vector/graph representation of floor plans        |\n",
            "| Possible Plots/Charts        | Model training/evaluation results (potential)     |\n",
            "\n",
            "**Conclusion:**  \n",
            "Based on the document excerpts, the visual elements present include at least one figure of the system’s user interface (Figure 7), annotated floor plan images from the dataset, and likely diagrams of vector/graph representations of these plans. There may also be charts or plots related to model performance or data statistics.\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --to notebook --inplace your_notebook.ipynb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz7ExE23gMnU",
        "outputId": "1fbecf93-21aa-415f-f83f-d9d107bbeafd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] WARNING | pattern 'your_notebook.ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb)\n",
            "        to various other formats.\n",
            "\n",
            "        WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "=======\n",
            "The options below are convenience aliases to configurable class-options,\n",
            "as listed in the \"Equivalent to\" description-line of the aliases.\n",
            "To see all configurable class-options for some <cmd>, use:\n",
            "    <cmd> --help-all\n",
            "\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "    Equivalent to: [--Application.log_level=10]\n",
            "--show-config\n",
            "    Show the application's configuration (human-readable format)\n",
            "    Equivalent to: [--Application.show_config=True]\n",
            "--show-config-json\n",
            "    Show the application's configuration (json format)\n",
            "    Equivalent to: [--Application.show_config_json=True]\n",
            "--generate-config\n",
            "    generate default config file\n",
            "    Equivalent to: [--JupyterApp.generate_config=True]\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "    Equivalent to: [--JupyterApp.answer_yes=True]\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "    Equivalent to: [--ExecutePreprocessor.enabled=True]\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "    Equivalent to: [--ExecutePreprocessor.allow_errors=True]\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "    Equivalent to: [--NbConvertApp.from_stdin=True]\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "    Equivalent to: [--NbConvertApp.writer_class=StdoutWriter]\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only\n",
            "            relevant when converting to notebook format)\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory=]\n",
            "--clear-output\n",
            "    Clear output of current file and save in place,\n",
            "            overwriting the existing notebook.\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --ClearOutputPreprocessor.enabled=True]\n",
            "--coalesce-streams\n",
            "    Coalesce consecutive stdout and stderr outputs into one stream (within each cell).\n",
            "    Equivalent to: [--NbConvertApp.use_output_suffix=False --NbConvertApp.export_format=notebook --FilesWriter.build_directory= --CoalesceStreamsPreprocessor.enabled=True]\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input_prompt=True --TemplateExporter.exclude_output_prompt=True]\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document.\n",
            "            This mode is ideal for generating code-free reports.\n",
            "    Equivalent to: [--TemplateExporter.exclude_output_prompt=True --TemplateExporter.exclude_input=True --TemplateExporter.exclude_input_prompt=True]\n",
            "--allow-chromium-download\n",
            "    Whether to allow downloading chromium if no suitable version is found on the system.\n",
            "    Equivalent to: [--WebPDFExporter.allow_chromium_download=True]\n",
            "--disable-chromium-sandbox\n",
            "    Disable chromium security sandbox when converting to PDF..\n",
            "    Equivalent to: [--WebPDFExporter.disable_sandbox=True]\n",
            "--show-input\n",
            "    Shows code input. This flag is only useful for dejavu users.\n",
            "    Equivalent to: [--TemplateExporter.exclude_input=False]\n",
            "--embed-images\n",
            "    Embed the images as base64 dataurls in the output. This flag is only useful for the HTML/WebPDF/Slides exports.\n",
            "    Equivalent to: [--HTMLExporter.embed_images=True]\n",
            "--sanitize-html\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized..\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html=True]\n",
            "--log-level=<Enum>\n",
            "    Set the log level by value or name.\n",
            "    Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']\n",
            "    Default: 30\n",
            "    Equivalent to: [--Application.log_level]\n",
            "--config=<Unicode>\n",
            "    Full path of a config file.\n",
            "    Default: ''\n",
            "    Equivalent to: [--JupyterApp.config_file]\n",
            "--to=<Unicode>\n",
            "    The export format to be used, either one of the built-in formats\n",
            "            ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf']\n",
            "            or a dotted object name that represents the import path for an\n",
            "            ``Exporter`` class\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.export_format]\n",
            "--template=<Unicode>\n",
            "    Name of the template to use\n",
            "    Default: ''\n",
            "    Equivalent to: [--TemplateExporter.template_name]\n",
            "--template-file=<Unicode>\n",
            "    Name of the template file to use\n",
            "    Default: None\n",
            "    Equivalent to: [--TemplateExporter.template_file]\n",
            "--theme=<Unicode>\n",
            "    Template specific theme(e.g. the name of a JupyterLab CSS theme distributed\n",
            "    as prebuilt extension for the lab template)\n",
            "    Default: 'light'\n",
            "    Equivalent to: [--HTMLExporter.theme]\n",
            "--sanitize_html=<Bool>\n",
            "    Whether the HTML in Markdown cells and cell outputs should be sanitized.This\n",
            "    should be set to True by nbviewer or similar tools.\n",
            "    Default: False\n",
            "    Equivalent to: [--HTMLExporter.sanitize_html]\n",
            "--writer=<DottedObjectName>\n",
            "    Writer class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: 'FilesWriter'\n",
            "    Equivalent to: [--NbConvertApp.writer_class]\n",
            "--post=<DottedOrNone>\n",
            "    PostProcessor class used to write the\n",
            "                                        results of the conversion\n",
            "    Default: ''\n",
            "    Equivalent to: [--NbConvertApp.postprocessor_class]\n",
            "--output=<Unicode>\n",
            "    Overwrite base name use for output files.\n",
            "                Supports pattern replacements '{notebook_name}'.\n",
            "    Default: '{notebook_name}'\n",
            "    Equivalent to: [--NbConvertApp.output_base]\n",
            "--output-dir=<Unicode>\n",
            "    Directory to write output(s) to. Defaults\n",
            "                                  to output to the directory of each notebook. To recover\n",
            "                                  previous default behaviour (outputting to the current\n",
            "                                  working directory) use . as the flag value.\n",
            "    Default: ''\n",
            "    Equivalent to: [--FilesWriter.build_directory]\n",
            "--reveal-prefix=<Unicode>\n",
            "    The URL prefix for reveal.js (version 3.x).\n",
            "            This defaults to the reveal CDN, but can be any url pointing to a copy\n",
            "            of reveal.js.\n",
            "            For speaker notes to work, this must be a relative path to a local\n",
            "            copy of reveal.js: e.g., \"reveal.js\".\n",
            "            If a relative path is given, it must be a subdirectory of the\n",
            "            current directory (from which the server is run).\n",
            "            See the usage documentation\n",
            "            (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-slideshow)\n",
            "            for more details.\n",
            "    Default: ''\n",
            "    Equivalent to: [--SlidesExporter.reveal_url_prefix]\n",
            "--nbformat=<Enum>\n",
            "    The nbformat version to write.\n",
            "            Use this to downgrade notebooks.\n",
            "    Choices: any of [1, 2, 3, 4]\n",
            "    Default: 4\n",
            "    Equivalent to: [--NotebookExporter.nbformat_version]\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to html\n",
            "\n",
            "            Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'qtpdf', 'qtpng', 'rst', 'script', 'slides', 'webpdf'].\n",
            "\n",
            "            > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "\n",
            "            Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "            'base', 'article' and 'report'.  HTML includes 'basic', 'lab' and\n",
            "            'classic'. You can specify the flavor of the format used.\n",
            "\n",
            "            > jupyter nbconvert --to html --template lab mynotebook.ipynb\n",
            "\n",
            "            You can also pipe the output to stdout, rather than a file\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "\n",
            "            PDF is generated via latex\n",
            "\n",
            "            > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "\n",
            "            You can get (and serve) a Reveal.js-powered slideshow\n",
            "\n",
            "            > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "\n",
            "            Multiple notebooks can be given at the command line in a couple of\n",
            "            different ways:\n",
            "\n",
            "            > jupyter nbconvert notebook*.ipynb\n",
            "            > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "\n",
            "            or you can specify the notebooks list in a config file, containing::\n",
            "\n",
            "                c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "\n",
            "            > jupyter nbconvert --config mycfg.py\n",
            "\n",
            "To see all available configurables, use `--help-all`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iNO-8SxrlDtK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}